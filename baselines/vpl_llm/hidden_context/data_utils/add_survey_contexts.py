# This file is used to preprocess dataset, available for any HH-RLHF format datasets
import os
from dataclasses import dataclass, field
from typing import Optional, cast
from tqdm import tqdm
import random

from transformers import (
    HfArgumentParser,
)

import torch

from hidden_context.train_llm_preference_model import (
    concatenate_datasets,
)

from hidden_context.data_utils.data_processing import generate_embeddings_with_llm
from datasets import load_dataset

from copy import deepcopy

import numpy as np

@dataclass
class ScriptArguments:

    output_dir: Optional[str] = field(
        metadata={"help": "Directory where the new dataset will be stored."},
    )
    data_path: str = field(
        metadata={"help": "Directory where the original data is stored."}
    )
    train_dataset_size: Optional[int] = field(
        default=64000,
        metadata={"help": "The size of the training dataset."},
    )
    eval_data_size: Optional[int] = field(
        default=6400,
        metadata={"help": "The size of the eval dataset."},
    )
    
    data_subset: str = field(
        default="helpful",
        metadata={
            "help": "Which subset of the data to use. You can choose between"
                    "'helpful', or 'harmless'."
        },
    )
    data_split: str = field(
        default="test",
        metadata={
            "help": "Which split of the data to use. You can choose between"
                    "'train', or 'test'."
        },
    )
    dataset_size: int = field(
        default=0,
        metadata={"help": "The size of the subset of the data to use"},
    )
    model_type: str = field(
        default="none",
        metadata={
            "help": "You can choose between 'gpt2', 'llama', or 'none'."
        }
    )
    embed_dim: int = field(
        default=1024,
        metadata={
            "help": "Dimension of the embeddings generated by LLM."
        }
    )
    max_length: int = field(
        default=1024,
        metadata={
            "help": "Maximum token length of the inputs."
        }
    )
    with_embeddings: bool = field(
        default=True,
        metadata={
            "help": "Whether the embeddings are generated during pre-processing."
        }
    )
    synthetic_dataset: bool = field(
        default=False,
        metadata={
            "help": "Whether a synthetic dataset is used."
        }
    )
    other_subsets: str = field(default=None)
    survey_size: int = field(
        default=8,
        metadata={
            "help": "Size of survey question pool."
        }
    )
    context_length: int = field(
        default=8,
        metadata={
            "help": "(Maximum) context length."
        }
    )
    controversial_only: bool = field(
        default=True,
        metadata={
            "help": "Whether to only generate controversial data points."
        }
    )
    num_duplicates: int = field(
        default=1,
        metadata={
            "help": "Number of times each data point repeatedly appears in the dataset (with resampled context)."
        }
    )
    fixed_context_length: bool = field(
        default=False,
        metadata={
            "help": "Whether to fix the context to the maximum length."
        }
    )
    random_contexts: bool = field(
        default=False,
        metadata={
            "help": "Whether to include controversial pairs in context."
        }
    )


# def generate_contexts(args, input_dataset, survey_dataset):
#     # Generate context with survey question pool
#     output_dir = os.path.join(args.output_dir, f"{args.model_type}", f"{args.data_subset}")
#     if not os.path.exists(output_dir):
#         os.makedirs(output_dir)

#     if args.controversial_only:
#         input_dataset = input_dataset.filter(lambda x: x['controversial'] == True)
#     dataset_size = len(input_dataset)
#     if args.data_split == 'train':
#         K = args.num_duplicates  # repeat samples for K times
#     else:
#         K = 1
#     dataset_list = list()

#     def random_choice(max_context_length, survey_size):
#         if max_context_length <= survey_size:
#             from functools import reduce
#             while True:
#                 if args.fixed_context_length:
#                     context_length = max_context_length
#                 else:
#                     if args.other_subsets == '84':
#                         context_length = random.randint(1, max_context_length)
#                     else:
#                         context_length = random.randint(2, max_context_length)
#                 context_chosen_ids = np.random.choice(survey_size, context_length, replace=False)
#                 chosen_dataset = [d for idx, d in enumerate(survey_dataset) if idx in context_chosen_ids]
#                 if args.other_subsets != 'single':
#                     return chosen_dataset, context_length
#                 satisfied_sets = list()
#                 for row in chosen_dataset:
#                     satisfied_sets.append(set(row["satisfied_subset"]))
#                 if len(reduce(lambda x, y: x.intersection(y), satisfied_sets)) == 1:
#                     return chosen_dataset, context_length
#                 elif context_length == survey_size:
#                     raise ValueError("Please choose another random seed!")
#         else:
#             raise ValueError("Context length is larger than survey size!")

#     for idx in range(K):
#         output_dataset = deepcopy(input_dataset)
#         context_lengths = list()
#         contexts = list()
#         for _ in tqdm(range(dataset_size)):  # iterate over all samples in original dataset
#             row_contexts = list()

#             context_dataset, context_length = random_choice(args.context_length, args.survey_size)
#             context_lengths.append(context_length)
#             for context_row in context_dataset:
#                 context_id = context_row["Index"]
#                 context_data = context_row
#                 if not args.with_embeddings:
#                     row_contexts.append({
#                         'original_id': context_id,
#                         'chosen': context_data['chosen'],
#                         'rejected': context_data['rejected'],
#                     })
#                 else:
#                     row_contexts.append({
#                         'original_id': context_id,
#                         'embedding_chosen': context_data['embeddings']['embedding_chosen'],
#                         'embedding_rejected': context_data['embeddings']['embedding_rejected'],
#                     })
#             contexts.append(row_contexts)
#         output_dataset = output_dataset.add_column("context_length", context_lengths)
#         output_dataset = output_dataset.add_column("contexts", contexts)
#         output_dataset.map()
#         dataset_list.append(output_dataset)
#     output = concatenate_datasets(dataset_list)
#     output.to_json(os.path.join(output_dir, f"{args.data_split}.jsonl"))
#     return output

def generate_contexts(args, input_dataset, survey_dataset):
    # Create output directory if it doesn't exist
    output_dir = os.path.join(args.output_dir, f"{args.model_type}", f"{args.data_subset}")
    os.makedirs(output_dir, exist_ok=True)

    # Filter input_dataset if necessary
    if args.controversial_only:
        input_dataset = input_dataset.filter(lambda x: x['controversial'] == True)

    dataset_size = len(input_dataset)
    K = args.num_duplicates if args.data_split == 'train' else 1
    dataset_list = []

    # Precompute survey indices
    survey_indices = np.arange(args.survey_size)

    # Define the function to generate context for a single sample
    def generate_sample_context(example, idx):
        while True:
            # Determine context length
            if args.fixed_context_length:
                context_length = args.context_length
            else:
                min_length = 1 if args.other_subsets == '84' else 2
                context_length = random.randint(min_length, args.context_length)

            # Randomly select context indices
            context_chosen_ids = np.random.choice(survey_indices, context_length, replace=False)
            context_dataset = survey_dataset.select(context_chosen_ids.tolist())

            # Check conditions for 'single' other_subsets
            if args.other_subsets == 'single':
                satisfied_sets = [set(row["satisfied_subset"]) for row in context_dataset]
                intersection = set.intersection(*satisfied_sets)
                if len(intersection) == 1:
                    break
                elif context_length == args.survey_size:
                    raise ValueError("Please choose another random seed!")
            else:
                break

        # Build context data
        row_contexts = []
        for context_row in context_dataset:
            context_id = context_row["Index"]
            if not args.with_embeddings:
                row_contexts.append({
                    'original_id': context_id,
                    'chosen': context_row['chosen'],
                    'rejected': context_row['rejected'],
                })
            else:
                row_contexts.append({
                    'original_id': context_id,
                    'embedding_chosen': context_row['embeddings']['embedding_chosen'],
                    'embedding_rejected': context_row['embeddings']['embedding_rejected'],
                })
        return {'context_length': context_length, 'contexts': row_contexts}

    # Generate contexts using the map function with multiprocessing
    for i in range(K):
        print('generating contexts for duplicate', i)
        output_dataset = input_dataset.map(
            generate_sample_context,
            with_indices=True,
            num_proc=os.cpu_count()
        )
        dataset_list.append(output_dataset)

    # Concatenate datasets and save to JSONL
    output = concatenate_datasets(dataset_list)
    print(f"saving to {os.path.join(output_dir, f'{args.data_split}.jsonl')}")
    output.to_json(os.path.join(output_dir, f"{args.data_split}.jsonl"))
    return output


if __name__ == "__main__":
    # default setting on HH-RLHF dataset, please iterate over data subsets and data splits
    seed = 0
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    parser = HfArgumentParser(ScriptArguments)
    script_args: ScriptArguments = parser.parse_args_into_dataclasses()[0]
    print(script_args)
    
    print('generating embeddings')
    dataset = generate_embeddings_with_llm(script_args)
    print("generated embeddings")
    dataset = dataset.map(lambda x: {"survey_options": True}, num_proc=4)
    if not script_args.random_contexts:
        survey_options = dataset.filter(lambda x: x['survey_options'] == True)
    else:
        survey_options = dataset.filter(lambda x: x['survey_options'] == True or x['survey_options'] == False)
    survey_ids = np.random.choice(range(len(survey_options)), script_args.survey_size, replace=False)
    print(survey_ids)
    if script_args.data_split == "train":
        survey_data = survey_options.filter(lambda example, idx: idx in survey_ids, with_indices=True)
        survey_data.to_json(os.path.join(script_args.data_path, script_args.data_subset, "survey_{}.jsonl".format(script_args.survey_size)))
    else:
        survey_data = load_dataset('json', data_files=os.path.join(script_args.data_path, script_args.data_subset, "survey_{}.jsonl".format(script_args.survey_size)))
        survey_data = survey_data['train']
    print('generating context and saving')
    generate_contexts(script_args, dataset, survey_data)
    print('done')