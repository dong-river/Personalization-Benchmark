# This file is used to preprocess dataset, available for any HH-RLHF format datasets
import os
from dataclasses import dataclass, field
from typing import Optional, cast
from tqdm import tqdm
import random

from transformers import (
    HfArgumentParser,
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
)

import torch

from hidden_context.train_llm_preference_model import (
    DataSubset,
    get_hh_rlhf_dataset,
    concatenate_datasets,
    HHRLHFPreprocessor,
)

from copy import deepcopy

import numpy as np
from datasets import load_dataset

@dataclass
class ScriptArguments:
    output_dir: Optional[str] = field(
        metadata={"help": "Directory where the new dataset will be stored."},
    )
    data_path: str = field(
        metadata={"help": "Directory where the original data is stored."}
    )
    data_subset: str = field(
        default="helpful",
        metadata={
            "help": "Which subset of the data to use. You can choose between"
                    "'helpful', or 'harmless'."
        },
    )
    data_split: str = field(
        default="test",
        metadata={
            "help": "Which split of the data to use. You can choose between"
                    "'train', or 'test'."
        },
    )
    dataset_size: int = field(
        default=0,
        metadata={"help": "The size of the subset of the data to use"},
    )
    model_type: str = field(
        default="none",
        metadata={
            "help": "You can choose between 'gpt2', 'llama', or 'none'."
        }
    )
    embed_dim: int = field(
        default=1024,
        metadata={
            "help": "Dimension of the embeddings generated by LLM."
        }
    )
    max_length: int = field(
        default=1024,
        metadata={
            "help": "Maximum token length of the inputs."
        }
    )
    with_embeddings: bool = field(
        default=True,
        metadata={
            "help": "Whether the embeddings are generated during pre-processing."
        }
    )
    synthetic_dataset: bool = field(
        default=False,
        metadata={
            "help": "Whether a synthetic dataset is used."
        }
    )
    other_subsets: str = field(default=None)

def build_vanilla_ultrafeedback_p_dataset(script_args, tokenizer, data_subset = None, return_tokenized=True, prepend_idx=False, subset="full"):
    def tokenize(sample):                    
        sample['positive'] = tokenizer.apply_chat_template(
            sample['chosen'], tokenize=False, add_generation_prompt=False).replace(tokenizer.bos_token, "")
        sample['negative'] = tokenizer.apply_chat_template(
            sample['rejected'], tokenize=False, add_generation_prompt=False).replace(tokenizer.bos_token, "")
        
        tokenized_pos = tokenizer(sample['positive'], truncation=True)
        tokenized_neg = tokenizer(sample['negative'], truncation=True)
        sample["input_ids_chosen"] = tokenized_pos["input_ids"]
        sample["attention_mask_chosen"] = tokenized_pos["attention_mask"]
        sample["input_ids_rejected"] = tokenized_neg["input_ids"]
        sample["attention_mask_rejected"] = tokenized_neg["attention_mask"]
        sample["uid"] = sample["uid"]
        return sample
    
    reverse_metric_map = {
        "instruction_following": 0,
        "honesty": 1,
        "truthfulness": 2,
        "helpfulness": 3,
    }

    print("Loading UltraFeedback dataset")   
    if subset == "controversial":
        train_dataset = load_dataset("RiverDong/ultrafeedback-p", subset, split='train')
        test_dataset = load_dataset("RiverDong/ultrafeedback-p", subset, split='test')
    elif subset == 'default':
        train_dataset = load_dataset("RiverDong/ultrafeedback-p", subset, split='train')
        test_dataset = load_dataset("RiverDong/ultrafeedback-p", subset, split='test')
    elif subset == 'ood':
        train_dataset = load_dataset("RiverDong/ultrafeedback-p", subset, split='train')
        test_dataset = load_dataset("RiverDong/ultrafeedback-p", subset, split='test')
    elif subset == 'ood-controversial':
        train_dataset = load_dataset("RiverDong/ultrafeedback-p", subset, split='train')
        test_dataset = load_dataset("RiverDong/ultrafeedback-p", subset, split='test')
    else:
        raise ValueError(f"Invalid subset: {subset}")

    train_dataset = train_dataset.select(range(min(script_args.train_dataset_size, len(train_dataset))))
    test_dataset = test_dataset.select(range(min(script_args.eval_data_size, len(test_dataset))))
    
    ## set chosen and rejected to the correct format
    train_dataset = train_dataset.rename_column("chosen", "chosen_only")
    train_dataset = train_dataset.rename_column("rejected", "rejected_only")
    test_dataset = test_dataset.rename_column("chosen", "chosen_only")
    test_dataset = test_dataset.rename_column("rejected", "rejected_only")
    
    
    train_dataset = train_dataset.map(lambda x: {"uid": reverse_metric_map[x["attributes"]]}, remove_columns=["attributes"])
    test_dataset = test_dataset.map(lambda x: {"uid": reverse_metric_map[x["attributes"]]}, remove_columns=["attributes"])
        
    train_dataset = train_dataset.map(lambda x: {"chosen": [{"content": x["prompt"], "role": "user"}, {"content": x["chosen_only"], "role": "assistant"}]})
    train_dataset = train_dataset.map(lambda x: {"rejected": [{"content": x["prompt"], "role": "user"}, {"content": x["rejected_only"], "role": "assistant"}]})
    test_dataset = test_dataset.map(lambda x: {"chosen": [{"content": x["prompt"], "role": "user"}, {"content": x["chosen_only"], "role": "assistant"}]})
    test_dataset = test_dataset.map(lambda x: {"rejected": [{"content": x["prompt"], "role": "user"}, {"content": x["rejected_only"], "role": "assistant"}]})
    

    train_dataset = train_dataset.filter(lambda x: x['prompt'] is not None and x['chosen'] is not None and x['rejected'] is not None)
    test_dataset = test_dataset.filter(lambda x: x['prompt'] is not None and x['chosen'] is not None and x['rejected'] is not None)
        
    if return_tokenized:
        train_dataset = train_dataset.map(tokenize, num_proc=16)
        test_dataset = test_dataset.map(tokenize, num_proc=16)

    if data_subset == 'all' and data_subset is not None:
        train_dataset = train_dataset.filter(lambda x: x['data_subset'] == data_subset)
        test_dataset = test_dataset.filter(lambda x: x['data_subset'] == data_subset)
    
    print("Training set: ", len(train_dataset), " test set: ", len(test_dataset))
    ## dataset has column: prompt, chosen, rejected, chosen_only, rejected_only, uid
    return train_dataset, test_dataset

def generate_embeddings_with_llm(args, input_dataset=None):
    """
    This function is used to generate fixed embeddings for inputs from original dataset.
    """
    if not args.synthetic_dataset:
        data_subset = cast(DataSubset, args.data_subset)

    if args.model_type == "gpt2":
        tokenizer = AutoTokenizer.from_pretrained("gpt2", use_auth_token=True)
        model = AutoModelForSequenceClassification.from_pretrained(
            "gpt2", num_labels=args.embed_dim, torch_dtype=torch.bfloat16
        )
        model.score.weight.data *= 0.01
    elif args.model_type == "llama" or args.model_type == "meta-llama/Llama-2-7b-hf":
        tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf", use_auth_token=True, add_eos_token=False)
        model = AutoModelForCausalLM.from_pretrained(
            "meta-llama/Llama-2-7b-hf", torch_dtype=torch.bfloat16
        )
    else:
        return input_dataset
    model.to("cuda")

    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.pad_token_id = tokenizer.eos_token_id
    tokenizer.padding_side = "right"

    model.config.pad_token_id = tokenizer.pad_token_id


    train_dataset, test_dataset = build_vanilla_ultrafeedback_p_dataset(args, tokenizer, data_subset = args.data_subset, return_tokenized=False, subset='controversial')
    input_dataset = concatenate_datasets([train_dataset, test_dataset])

    train_dataset_tokenized, test_dataset_tokenized = build_vanilla_ultrafeedback_p_dataset(args, tokenizer, data_subset = args.data_subset, return_tokenized=True, subset='controversial')
    preprocessed_dataset = concatenate_datasets([train_dataset_tokenized, test_dataset_tokenized])
    print(len(preprocessed_dataset))
    
    input_dataset = input_dataset.filter(
        lambda example, idx: len(preprocessed_dataset[idx]["input_ids_chosen"]) <= args.max_length
                             and len(preprocessed_dataset[idx]["input_ids_rejected"]) <= args.max_length,
        with_indices=True
    )
    preprocessed_dataset = preprocessed_dataset.filter(
        lambda example: len(example["input_ids_chosen"]) <= args.max_length
                        and len(example["input_ids_rejected"]) <= args.max_length
    )
    # print(len(input_dataset), len(preprocessed_dataset))
    dataset_size = len(preprocessed_dataset)
    
    print("Generating embeddings")
    embeddings = list()
    for row_id in tqdm(range(dataset_size)):
        emb = dict()
        for key in ['chosen', 'rejected']:
            tokens = tokenizer.pad(
                {"input_ids": preprocessed_dataset[row_id][f"input_ids_{key}"]},
                padding=True, pad_to_multiple_of=64, return_tensors="pt"
            )
            token_length = len(preprocessed_dataset[row_id][f"input_ids_{key}"])
            input_ids = tokens["input_ids"].unsqueeze(0).to("cuda")
            attention_mask = tokens["attention_mask"].unsqueeze(0).to("cuda")
            with torch.no_grad():
                last_hidden_state = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    output_hidden_states=True
                ).hidden_states[-1]
                emb[f"embedding_{key}"] = last_hidden_state[0][token_length - 1].float().cpu().numpy()
        embeddings.append(emb)
    output_dataset = input_dataset.add_column("embeddings", embeddings)
    return output_dataset


def generate_contexts(args, input_dataset):
    # Generate context without survey question pool
    output_dir = os.path.join(args.output_dir, f"{args.model_type}", f"{args.data_subset}")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    dataset_size = len(input_dataset)

    K = 1  # repeat samples for K times
    dataset_list = list()
    for idx in range(K):
        context_dataset = deepcopy(input_dataset)
        context_lengths = [8] * dataset_size
        if "context_length" in context_dataset.column_names:
            context_dataset = context_dataset.remove_columns("context_length")
        context_dataset = context_dataset.add_column("context_length", context_lengths)
        contexts = list()
        for row_id in tqdm(range(dataset_size)):  # iterate over all samples in original dataset
            row_contexts = list()
            num_context = 0
            controversial_subset = input_dataset.filter(lambda example: example['controversial'] == True)
            controversial_size = len(controversial_subset)
            while num_context < context_lengths[row_id]:
                random_id = np.random.randint(controversial_size)
                context_id = controversial_subset[random_id]['Index']
                context_data = controversial_subset[random_id]
                if not args.synthetic_dataset:
                    if input_dataset[row_id]['prompt'] == context_data['prompt']:
                        continue
                if not args.with_embeddings:
                    row_contexts.append({
                        'original_id': context_id,
                        'chosen': context_data['chosen'],
                        'rejected': context_data['rejected'],
                    })
                else:
                    row_contexts.append({
                        'original_id': context_id,
                        'embedding_chosen': context_data['embeddings']['embedding_chosen'],
                        'embedding_rejected': context_data['embeddings']['embedding_rejected'],
                    })
                num_context += 1
            contexts.append(row_contexts)
        context_dataset = context_dataset.add_column("contexts", contexts)
        dataset_list.append(context_dataset)

    output = concatenate_datasets(dataset_list)
    output.to_json(os.path.join(output_dir, f"{args.data_split}.jsonl"))
    return output


if __name__ == "__main__":
    # default setting on HH-RLHF dataset, please iterate over data subsets and data splits
    seed = 0
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    parser = HfArgumentParser(ScriptArguments)
    script_args: ScriptArguments = parser.parse_args_into_dataclasses()[0]
    print(script_args)
    dataset = generate_embeddings_with_llm(script_args)
    generate_contexts(script_args, dataset)







# def generate_embeddings_with_llm(args, input_dataset=None):
#     """
#     This function is used to generate fixed embeddings for inputs from original dataset.
#     """
#     if not args.synthetic_dataset:
#         data_subset = cast(DataSubset, args.data_subset)
#         # input_dataset = get_hh_rlhf_dataset(
#         #     data_subset,
#         #     args.data_split,
#         #     args.dataset_size,
#         #     data_path=args.data_path,
#         #     use_subset_as_dir=True,
#         #     other_subsets=args.other_subsets,
#         # )

#     if args.model_type == "gpt2":
#         tokenizer = AutoTokenizer.from_pretrained("gpt2", use_auth_token=True)
#         model = AutoModelForSequenceClassification.from_pretrained(
#             "gpt2", num_labels=args.embed_dim, torch_dtype=torch.bfloat16
#         )
#         model.score.weight.data *= 0.01
#     elif args.model_type == "llama" or args.model_type == "meta-llama/Llama-2-7b-hf":
#         tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf", use_auth_token=True, add_eos_token=False)
#         model = AutoModelForCausalLM.from_pretrained(
#             "meta-llama/Llama-2-7b-hf", torch_dtype=torch.bfloat16
#         )
#     else:
#         return input_dataset
#     model.to("cuda")

#     tokenizer.pad_token = tokenizer.eos_token
#     tokenizer.pad_token_id = tokenizer.eos_token_id
#     tokenizer.padding_side = "right"

#     model.config.pad_token_id = tokenizer.pad_token_id
#     # dataset_size = len(input_dataset)
#     # print(dataset_size)

#     train_dataset, test_dataset = build_vanilla_ultrafeedback_p_dataset(tokenizer, return_tokenized=False, subset='controversial')
#     input_dataset = concatenate_datasets([train_dataset, test_dataset])

#     train_dataset_tokenized, test_dataset_tokenized = build_vanilla_ultrafeedback_p_dataset(tokenizer, return_tokenized=True, subset='controversial')
#     preprocessed_dataset = concatenate_datasets([train_dataset_tokenized, test_dataset_tokenized])
#     print(len(preprocessed_dataset))
#     # preprocessed_dataset = input_dataset.map(
#     #     HHRLHFPreprocessor(tokenizer),
#     #     batched=True,
#     #     num_proc=24,
#     #     remove_columns=input_dataset.column_names,
#     #     load_from_cache_file=False,
#     # )

#     # input_dataset = input_dataset.filter(
#     #     lambda example, idx: len(preprocessed_dataset[idx]["input_ids_chosen"]) <= args.max_length
#     #                          and len(preprocessed_dataset[idx]["input_ids_rejected"]) <= args.max_length,
#     #     with_indices=True
#     # )
#     preprocessed_dataset = preprocessed_dataset.filter(
#         lambda example: len(example["input_ids_chosen"]) <= args.max_length
#                         and len(example["input_ids_rejected"]) <= args.max_length
#     )
#     # print(len(input_dataset), len(preprocessed_dataset))
#     dataset_size = len(preprocessed_dataset)
    
#     print("Generating embeddings")
#     embeddings = list()
#     for row_id in tqdm(range(dataset_size)):
#         emb = dict()
#         for key in ['chosen', 'rejected']:
#             tokens = tokenizer.pad(
#                 {"input_ids": preprocessed_dataset[row_id][f"input_ids_{key}"]},
#                 padding=True, pad_to_multiple_of=64, return_tensors="pt"
#             )
#             token_length = len(preprocessed_dataset[row_id][f"input_ids_{key}"])
#             input_ids = tokens["input_ids"].unsqueeze(0).to("cuda")
#             attention_mask = tokens["attention_mask"].unsqueeze(0).to("cuda")
#             with torch.no_grad():
#                 last_hidden_state = model(
#                     input_ids=input_ids,
#                     attention_mask=attention_mask,
#                     output_hidden_states=True
#                 ).hidden_states[-1]
#                 emb[f"embedding_{key}"] = last_hidden_state[0][token_length - 1].float().cpu().numpy()
#         embeddings.append(emb)
#     output_dataset = input_dataset.add_column("embeddings", embeddings)
#     return output_dataset