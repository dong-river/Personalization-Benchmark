{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from alpaca_farm.auto_annotations import PairwiseAutoAnnotator\n",
    "\n",
    "apikey = \"\"  # openai api key\n",
    "output_dir = \"\"  # the dir of generation json files\n",
    "dataset = \"prism\"  # or \"psoups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = apikey\n",
    "decoding_kwargs = dict(\n",
    "    openai_api_key = openai.api_key,\n",
    "    openai_organization_ids = None, # can set multiple orgs to avoid rate limits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"prism\":\n",
    "    with open(\"./prism_winrate_gpt_prompt.txt\", 'r') as f:\n",
    "        winrate_prompt = f.read()\n",
    "\n",
    "    with open (\"../data/prism_selected_examples.json\", 'r') as f:\n",
    "        selected_examples = json.load(f)\n",
    "\n",
    "    with open (\"../data/prism_data_dialog.json\", 'r') as f:\n",
    "        data_dialog = json.load(f)\n",
    "\n",
    "# for psoups evaluation by alcapa farm \n",
    "if dataset == \"psoups\":\n",
    "    user_annotator_mapping = {\"1\": \"p1a\", \"2\": \"p1b\", \"3\": \"p2a\", \"4\": \"p2b\", \"5\": \"p3a\", \"6\": \"p3b\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build entry for alpacafarm\n",
    "\n",
    "def split_output_text(output_text, input_text, dataset=\"prism\"):\n",
    "    if dataset == \"prism\":\n",
    "        assistant_split = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    elif dataset == \"psoups\":\n",
    "        assistant_split = \"<|assistant|>\\n\"\n",
    "\n",
    "    # if no new text, mark as wrong completion\n",
    "    if (len(output_text.split(assistant_split)) \n",
    "        < len(input_text.split(assistant_split))\n",
    "    ):\n",
    "        print(f\"Wrong completion!\")\n",
    "        completion = \"Wrong completion.\"\n",
    "    else:\n",
    "        completion = output_text.split(assistant_split)[-1]\n",
    "    return completion\n",
    "\n",
    "\n",
    "def to_annotate_format(filename_1, filename_2=\"chosen\", seed=123, dataset=\"prism\"):\n",
    "    # load outputs\n",
    "    with open(os.path.join(output_dir, filename_1), 'r') as f:\n",
    "        outputs_1 = json.load(f)\n",
    "    if filename_2 != \"chosen\":\n",
    "        with open(os.path.join(output_dir, filename_2), 'r') as f:\n",
    "            outputs_2 = json.load(f)\n",
    "        assert len(outputs_1) == len(outputs_2), \"must have the same number!\"\n",
    "    \n",
    "    # use labels to shuffle 1&2 position\n",
    "    np.random.seed(seed)\n",
    "    labels = np.random.choice(a=[1,2], size=len(outputs_1))\n",
    "    to_annotate = {\n",
    "        prompt_id: {\n",
    "            \"label\": labels[idx].item(),\n",
    "            \"comparisons\": []\n",
    "        } for idx, prompt_id in enumerate(list(outputs_1.keys()))\n",
    "    }\n",
    "\n",
    "    for prompt_id in outputs_1.keys():\n",
    "        if dataset == \"prism\":\n",
    "            user_id, dialog_id, turn_nb = prompt_id.split(\"_\")\n",
    "\n",
    "        # split completion from output text\n",
    "        completion_1 = split_output_text(outputs_1[prompt_id][\"output_text\"], outputs_1[prompt_id][\"input_text\"], dataset)\n",
    "        if filename_2 != \"chosen\":\n",
    "            completion_2 = split_output_text(outputs_2[prompt_id][\"output_text\"], outputs_2[prompt_id][\"input_text\"], dataset)\n",
    "        elif dataset == \"prism\":\n",
    "            completion_2 = selected_examples[dialog_id][\"turns\"][turn_nb][\"chosen_utterance\"]\n",
    "        else:\n",
    "            print(\"No chosen response for psoups! Please compare to sft generations.\")\n",
    "            return\n",
    "\n",
    "        if completion_1 == completion_2:\n",
    "            print(f\"{prompt_id}: same completions!\")\n",
    "\n",
    "        # get prompt\n",
    "        if dataset == \"prism\":\n",
    "            prompt = selected_examples[dialog_id][\"turns\"][turn_nb][\"history\"]\n",
    "        elif dataset == \"psoups\":\n",
    "            prompt = outputs_1[prompt_id][\"prompt\"].replace(\"<|user|>\\n\", \"\").replace(\" \\n<|assistant|>\\n\", \"\")\n",
    "\n",
    "        # conform to alpacafarm format\n",
    "        if to_annotate[prompt_id][\"label\"] == 1:\n",
    "            entry = {\n",
    "                \"instruction\": prompt,\n",
    "                \"input\": \"\",\n",
    "                \"output_1\": completion_1,\n",
    "                \"output_2\": completion_2,\n",
    "            }\n",
    "        elif to_annotate[prompt_id][\"label\"] == 2:\n",
    "            entry = {\n",
    "                \"instruction\": prompt,\n",
    "                \"input\": \"\",\n",
    "                \"output_1\": completion_2,\n",
    "                \"output_2\": completion_1,\n",
    "            }\n",
    "        \n",
    "        to_annotate[prompt_id][\"comparisons\"].append(entry)\n",
    "       \n",
    "    return to_annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for prism\n",
    "# generate and save prompt for each entry: replace <user_info>, <previous_choices>, <user_reasoning> in winrate_prompt\n",
    "def build_eval_prompts(to_annotate): \n",
    "    if not os.path.exists(\"./alpaca_farm/auto_annotations/prism_winrate_eval_prompts\"):\n",
    "        os.makedirs(\"./alpaca_farm/auto_annotations/prism_winrate_eval_prompts\")\n",
    "    np.random.seed(123)\n",
    "    labels = [1,2] * 500  # force the shuffle of response order in prompt\n",
    "    labels_idx = 0\n",
    "    for prompt_id in to_annotate.keys():\n",
    "        user_id, dialog_id, turn_nb = prompt_id.split(\"_\")\n",
    "        dialog = data_dialog[dialog_id]\n",
    "\n",
    "        # build <user_info>\n",
    "        user_info = \"\"\n",
    "        for k in selected_examples[dialog_id][\"demographics\"]:\n",
    "            if k != \"preference\":\n",
    "                user_info += f\"- {k.replace('_', ' ')}: {selected_examples[dialog_id]['demographics'][k]}\\n\"\n",
    "        eval_prompt = winrate_prompt.replace(\"<user_info>\", user_info[:-1])\n",
    "\n",
    "        # build <user_reasoning>\n",
    "        user_reasoning = f\"- Their expectation for the assistant: {selected_examples[dialog_id]['system_string']}\\n\"\n",
    "        user_reasoning += f\"- Their feedback on the chosen response: {selected_examples[dialog_id]['open_feedback']}\\n\"\n",
    "        preference = \", \".join(selected_examples[dialog_id][\"demographics\"][\"preference\"])\n",
    "        user_reasoning += f\"- Top three aspects of their preferences: {preference}\\n\"\n",
    "        eval_prompt = eval_prompt.replace(\"<user_reasoning>\", user_reasoning[:-1])\n",
    "\n",
    "        # build <previous_choices>\n",
    "        previous_choices = f\"a batch of {len(dialog['turns'])-1} examples.\\n\\n\"\n",
    "        num = 1\n",
    "        for idx, turn in enumerate(dialog[\"turns\"]):\n",
    "            if turn[\"turn_nb\"] == int(turn_nb):  # skip the turn to be annotated\n",
    "                continue\n",
    "            if turn['rejected_utterance'] == \"\":  # tie\n",
    "                example = f\"### Example {num}\\n\\n#### Instruction {num}:\\n{turn['user_utterance']}\\n\\n#### Output (a) for example {num}:\\n{turn['chosen_utterance']}\\n\\n#### Output (b) for example {num}:\\n{turn['chosen_utterance']}\\n\\n#### Result for example {num}:\\nTIE\\n\\n\"\n",
    "            elif labels[labels_idx] == 1:  # chosen in a\n",
    "                example = f\"### Example {num}\\n\\n#### Instruction {num}:\\n{turn['user_utterance']}\\n\\n#### Output (a) for example {num}:\\n{turn['chosen_utterance']}\\n\\n#### Output (b) for example {num}:\\n{turn['rejected_utterance']}\\n\\n#### Result for example {num}:\\nOutput (a)\\n\\n\"\n",
    "                labels_idx += 1\n",
    "            elif labels[labels_idx] == 2:  # chosen in b\n",
    "                example = f\"### Example {num}\\n\\n#### Instruction {num}:\\n{turn['user_utterance']}\\n\\n#### Output (a) for example {num}:\\n{turn['rejected_utterance']}\\n\\n#### Output (b) for example {num}:\\n{turn['chosen_utterance']}\\n\\n#### Result for example {num}:\\nOutput (b)\\n\\n\"\n",
    "                labels_idx += 1\n",
    "            previous_choices += example\n",
    "            num += 1\n",
    "        eval_prompt = eval_prompt.replace(\"<previous_examples>\", previous_choices[:-2])\n",
    "\n",
    "        with open(f\"./alpaca_farm/auto_annotations/prism_winrate_eval_prompts/{prompt_id}.txt\", 'w') as f:\n",
    "            f.write(eval_prompt)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for prism\n",
    "# build list of annotator config dict for each entry\n",
    "\n",
    "def build_annotator_configs(to_annotate, gptmodel=\"gpt-4o-mini\"):\n",
    "    configs = {}\n",
    "    for prompt_id in to_annotate.keys():\n",
    "        config = {}\n",
    "        config[\"prompt_templates\"] = {\"without_inputs\": f\"./prism_winrate_eval_prompts/{prompt_id}.txt\"}\n",
    "        config[\"fn_decoder\"] = \"openai_completions\"\n",
    "        config[\"decoder_kwargs\"] = {\"model_name\": gptmodel,\n",
    "                                    \"max_tokens\": 50,\n",
    "                                    \"temperature\": 0,\n",
    "                                    \"tokens_to_avoid\": ['Both', 'Neither', 'None', ' Both', ' Neither', 'Either', 'depends', 'context','It', 'both','Sorry'],\n",
    "                                    \"tokens_to_favor\": [\"Output (a)\", \"Output (b)\", \"TIE\"]\n",
    "                                }\n",
    "        config[\"outputs_to_match\"] = {  1: '(?:^|\\n|: )Output \\(a\\)',\n",
    "                                        2: '(?:^|\\n|: )Output \\(b\\)',\n",
    "                                        3: 'TIE'\n",
    "                                    }\n",
    "        config[\"batch_size\"] = 1\n",
    "        configs[prompt_id] = {prompt_id: config}\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run win-rate annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please assign the following variables\n",
    "gptmodel = \"gpt-4o-mini\"  # \"gpt-3.5-turbo-0125\" \"gpt-4o-2024-08-06\"\n",
    "filename_1 = \"\"  # the generation json files to be evaluated\n",
    "filename_2 = \"\"  # will compare file1 against file2\n",
    "\n",
    "if dataset == \"psoups\":\n",
    "    print(\"Use default gptmodel as in psoups paper&code: GPT-4 !\")\n",
    "\n",
    "to_annotate = to_annotate_format(filename_1, filename_2, dataset=dataset)\n",
    "if dataset == \"prism\":\n",
    "    build_eval_prompts(to_annotate)\n",
    "    configs = build_annotator_configs(to_annotate, gptmodel)\n",
    "\n",
    "print(len(to_annotate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate\n",
    "num = 0\n",
    "if not os.path.exists(f\"./{dataset}_winrate_eval_results/{gptmodel}_{filename_1}_{filename_2}\"):\n",
    "    os.makedirs(f\"./{dataset}_winrate_eval_results/{gptmodel}_{filename_1}_{filename_2}\")\n",
    "\n",
    "for prompt_id, sample in list(to_annotate.items()):\n",
    "    if dataset == \"prism\":\n",
    "       annotator = PairwiseAutoAnnotator(\n",
    "            annotators_config = configs[prompt_id],\n",
    "            saving_path = f\"./{dataset}_winrate_eval_results/{gptmodel}_{filename_1}_{filename_2}/{prompt_id}.json\", \n",
    "            is_avoid_reannotations = False,\n",
    "            **decoding_kwargs\n",
    "        )\n",
    "    elif dataset == \"psoups\":\n",
    "        user_id, sample_id = prompt_id.split(\"_\")\n",
    "        if user_id == '0':  # only evaluate user 1-6\n",
    "            continue\n",
    "        pref = user_annotator_mapping[user_id]\n",
    "        annotator = PairwiseAutoAnnotator(\n",
    "            annotators_config = f\"annotators/criteria_wise_eval_gpt4/{pref}.yaml\",\n",
    "            saving_path = f\"./{dataset}_winrate_eval_results/{gptmodel}_{filename_1}_{filename_2}/{prompt_id}.json\", \n",
    "            is_avoid_reannotations = False,\n",
    "            **decoding_kwargs\n",
    "        )\n",
    "    \n",
    "    to_annotate[prompt_id][\"annotation\"] = annotator.annotate_pairs(to_annotate[prompt_id][\"comparisons\"])\n",
    "    num += 1\n",
    "\n",
    "with open(f\"./{dataset}_winrate_eval_results/{gptmodel}_{filename_1}_{filename_2}.json\", \"w\") as f:\n",
    "    json.dump(to_annotate, f, indent=4)\n",
    "\n",
    "len(to_annotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# win-rate\n",
    "win_lose_tie = [0, 0, 0]\n",
    "per_user_win_lose_tie = {}\n",
    "per_user_winrate = []\n",
    "\n",
    "for prompt_id in to_annotate.keys():\n",
    "    if dataset == \"prism\":\n",
    "        user_id, dialog_id, turn_nb = prompt_id.split(\"_\")\n",
    "    elif dataset == \"psoups\":\n",
    "        user_id, sample_id = prompt_id.split(\"_\")\n",
    "        if user_id == '0':  # only evaluate user 1-6\n",
    "            continue\n",
    "        \n",
    "    if user_id not in per_user_win_lose_tie.keys():\n",
    "        per_user_win_lose_tie[user_id] = [0, 0, 0]\n",
    "\n",
    "    try:\n",
    "        annotation = to_annotate[prompt_id][\"annotation\"][0]\n",
    "    except:\n",
    "        print(\"no annotation for \", prompt_id)\n",
    "        continue\n",
    "\n",
    "    if int(annotation[\"preference\"]) == 3:\n",
    "        win_lose_tie[2] += 1\n",
    "        per_user_win_lose_tie[user_id][2] += 1\n",
    "    elif int(annotation[\"preference\"]) == to_annotate[prompt_id][\"label\"]:\n",
    "        win_lose_tie[0] += 1\n",
    "        per_user_win_lose_tie[user_id][0] += 1\n",
    "    elif int(annotation[\"preference\"]) != to_annotate[prompt_id][\"label\"]:\n",
    "        win_lose_tie[1] += 1\n",
    "        per_user_win_lose_tie[user_id][1] += 1\n",
    "\n",
    "if dataset == \"prism\":\n",
    "    # consider TIE as both sides win\n",
    "    for user_id in per_user_win_lose_tie.keys():\n",
    "        per_user_winrate.append((per_user_win_lose_tie[user_id][0]+per_user_win_lose_tie[user_id][2])/(per_user_win_lose_tie[user_id][0]+per_user_win_lose_tie[user_id][2]+per_user_win_lose_tie[user_id][1]+per_user_win_lose_tie[user_id][2]))\n",
    "    print(\"macro avg: \", win_lose_tie, (win_lose_tie[0]+win_lose_tie[2])/(win_lose_tie[0]+win_lose_tie[2]+win_lose_tie[1]+win_lose_tie[2]))\n",
    "    print(\"per user avg: \", sum(per_user_winrate)/len(per_user_winrate))\n",
    "\n",
    "elif dataset == \"psoups\":\n",
    "    # not consider TIE, to be consistent with the win-rate computation in the Personalized Soups paper\n",
    "    for user_id in per_user_win_lose_tie.keys():\n",
    "        per_user_winrate.append(per_user_win_lose_tie[user_id][0]/(per_user_win_lose_tie[user_id][0]+per_user_win_lose_tie[user_id][1]+per_user_win_lose_tie[user_id][2]))\n",
    "        print(\"user \", user_id, \" winrate: \", per_user_winrate[-1])\n",
    "    print(\"macro avg: \", win_lose_tie, win_lose_tie[0]/(win_lose_tie[0]+win_lose_tie[1]+win_lose_tie[2]))\n",
    "    print(\"per user avg: \", sum(per_user_winrate)/len(per_user_winrate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
